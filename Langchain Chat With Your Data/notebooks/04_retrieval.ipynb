{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f50764b6",
   "metadata": {
    "height": 30
   },
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3d1491f",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6039c9e9",
   "metadata": {},
   "source": [
    "## Load persisted vectordb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe7c4e6d",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0300cbcf",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "persist_directory = \"../data/chroma/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dca16928",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fb8487b",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb._collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97639c7",
   "metadata": {
    "height": 30
   },
   "source": [
    "## Small DB from texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a448efc0",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"\"\"The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\"\"\",\n",
    "    \"\"\"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\"\"\",\n",
    "    \"\"\"A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03132479",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "smalldb = Chroma.from_texts(texts, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "378edb26",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "question = \"Tell me about all-white mushrooms with large fruiting bodies\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f24385",
   "metadata": {},
   "source": [
    "### SS: Similarity Search\n",
    "\n",
    "Redundancy in resulted info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb28fdd7",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.', metadata={}),\n",
       " Document(page_content='The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).', metadata={})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smalldb.similarity_search(question, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f60d9d",
   "metadata": {},
   "source": [
    "### MMR: Maximum Marginal Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7edf4d76",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.', metadata={}),\n",
       " Document(page_content='A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.', metadata={})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smalldb.max_marginal_relevance_search(question, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c611b0a",
   "metadata": {},
   "source": [
    "## Addressing Diversity: Maximum marginal Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70198025",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "question = \"What did they say about matlab?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e753c3a1",
   "metadata": {},
   "source": [
    "### SS\n",
    "\n",
    "Duplicate documents feteched -> Redundancy in retrieved information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "521c7983",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "docs_ss = vectordb.similarity_search(question, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b76dab7",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people call it a free ve rsion of MATLAB, which it sort  of is, sort of isn't.  \\nSo I guess for those of you that haven't s een MATLAB before, and I know most of you \\nhave, MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to \\nplot data. And it's sort of an extremely easy to  learn tool to u\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_ss[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ff16703",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people call it a free ve rsion of MATLAB, which it sort  of is, sort of isn't.  \\nSo I guess for those of you that haven't s een MATLAB before, and I know most of you \\nhave, MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to \\nplot data. And it's sort of an extremely easy to  learn tool to u\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_ss[1].page_content[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69a9026",
   "metadata": {},
   "source": [
    "### MMR\n",
    "\n",
    "Maximum diversity in retrieved information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1265735",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "docs_mmr = vectordb.max_marginal_relevance_search(question, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad5f7690",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people call it a free ve rsion of MATLAB, which it sort  of is, sort of isn't.  \\nSo I guess for those of you that haven't s een MATLAB before, and I know most of you \\nhave, MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to \\nplot data. And it's sort of an extremely easy to  learn tool to u\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_mmr[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b9b0d24",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'algorithm then? So what’s different? How come  I was making all that noise earlier about \\nleast squares regression being a bad idea for classification problems and then I did a \\nbunch of math and I skipped some steps, but I’m, sort of, claiming at the end they’re \\nreally the same learning algorithm?  \\nStudent: [Inaudible] constants?  \\nInstructor (Andrew Ng) :Say that again.  \\nStudent: [Inaudible]  \\nInstructor (Andrew Ng) :Oh, right. Okay, cool.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_mmr[1].page_content[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb675c6e",
   "metadata": {},
   "source": [
    "## Addressing Specifity: working with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff5dfd",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb70c315",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vdb_doc = vectordb._collection.peek(1)\n",
    "type(vdb_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f1b8e37",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'embeddings', 'documents', 'metadatas'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vdb_doc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82c0299a",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': '../data/docs/cs229_lectures/MachineLearning-Lecture02.pdf',\n",
       "  'page': 11}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vdb_doc['metadatas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db435214",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "question = \"What did they say about regression in lecture 3?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5148dfed",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "docs = vectordb.similarity_search(\n",
    "    question,\n",
    "    k=3,\n",
    "    filter={'source': '../data/docs/cs229_lectures/MachineLearning-Lecture03.pdf'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3138c64a",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': '../data/docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 0}\n",
      "{'source': '../data/docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 14}\n",
      "{'source': '../data/docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 4}\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa15accb",
   "metadata": {},
   "source": [
    "## Addressing Specificity: working with metadata using `SelfQueryRetriever`\n",
    "\n",
    "\n",
    "`SelfQueryRetriever`, uses an LLM to extract:\n",
    "\n",
    "1. The `query` string to use for vector search\n",
    "2. A metadata filter to pass in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "920a3bc2",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.query_constructor.base import AttributeInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fd200b2",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24880fa8",
   "metadata": {
    "height": 268
   },
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name='source', \n",
    "        description=\"\"\"The lecture the document belongs to, should be one of `../data/docs/cs229_lectures/MachineLearning-Lecture01.pdf`, `../data/docs/cs229_lectures/MachineLearning-Lecture02.pdf`, or `../data/docs/cs229_lectures/MachineLearning-Lecture03.pdf`\"\"\",\n",
    "        type=\"string\"\n",
    "    ),\n",
    "    \n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"\"\"\n",
    "        The page number the document chunk belongs to\n",
    "        \"\"\",\n",
    "        type=\"integer\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f94fe19",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "doc_content_desc = \"Machine Learning Lecture Notes\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d6df1b8",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm=llm,\n",
    "    vectorstore=vectordb,\n",
    "    document_contents=doc_content_desc,\n",
    "    metadata_field_info=metadata_field_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e07af397",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "question = \"What did they say about regression in the third lecture?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af58f3f2",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/langchain/chains/llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a10bd1ac",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Student: It’s the lowest it –  \\nInstructor (Andrew Ng) :No, exactly. Right. So zero to the same, this is not the same, \\nright? And the reason is, in logi stic regression this is diffe rent from before, right? The \\ndefinition of this H subscript theta of XI is not the same as the definition I was using in \\nthe previous lecture. And in pa rticular this is no longer thet a transpose XI. This is not a \\nlinear function anymore. This is  a logistic function of theta transpose XI. Okay? So even \\nthough this looks cosmetically similar, even though this is similar on the surface, to the \\nBastrian descent rule I derive d last time for least squares regression this is actually a \\ntotally different learning algorithm. Okay? And it turns out that there’s actually no \\ncoincidence that you ended up with the same l earning rule. We’ll actually talk a bit more \\nabout this later when we talk about generalized linear models. But this is one of the most \\nelegant generalized learning models that we’l l see later. That even though we’re using a \\ndifferent model, you actually ended up with wh at looks like the sa me learning algorithm \\nand it’s actually no coincidence. Cool.  \\nOne last comment as part of a sort of l earning process, over here I said I take the \\nderivatives and I ended up with this line . I didn’t want to make you sit through a long \\nalgebraic derivation, but later t oday or later this week, pleas e, do go home and look at our', metadata={'source': '../data/docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 14}),\n",
       " Document(page_content='MachineLearning-Lecture03  \\nInstructor (Andrew Ng) :Okay. Good morning and welcome b ack to the third lecture of \\nthis class. So here’s what I want to do t oday, and some of the topics I do today may seem \\na little bit like I’m jumping, sort  of, from topic to topic, but here’s, sort of, the outline for \\ntoday and the illogical flow of ideas. In the last lecture, we  talked about linear regression \\nand today I want to talk about sort of an  adaptation of that called locally weighted \\nregression. It’s very a popular  algorithm that’s actually one of my former mentors \\nprobably favorite machine learning algorithm.  \\nWe’ll then talk about a probabl e second interpretation of linear regression and use that to \\nmove onto our first classification algorithm, which is logistic regr ession; take a brief \\ndigression to tell you about something cal led the perceptron algorithm, which is \\nsomething we’ll come back to, again, later this  quarter; and time allowing I hope to get to \\nNewton’s method, which is an algorithm fo r fitting logistic regression models.  \\nSo this is recap where we’re talking about in the previous lecture, remember the notation \\nI defined was that I used this X superscrip t I, Y superscript I to denote the I training \\nexample. And when we’re talking about linear regression or linear l east squares, we use \\nthis to denote the predicted value of “by my hypothesis H” on the input XI. And my', metadata={'source': '../data/docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 0}),\n",
       " Document(page_content='Instructor (Andrew Ng) :Yeah, yeah. I mean, you’re asking about overfitting, whether \\nthis is a good model. I thi nk let’s – the thing’s you’re mentioning are maybe deeper \\nquestions about learning algorithms  that we’ll just come back to later, so don’t really \\nwant to get into that right now. Any more questions? Okay.  \\nSo this endows linear regression with a proba bilistic interpretati on. I’m actually going to \\nuse this probabil – use this, sort of, probabilist ic interpretation in order to derive our next \\nlearning algorithm, which will be our first classification algorithm. Okay? So you’ll recall \\nthat I said that regression problems are where the variable Y that you’re trying to predict \\nis continuous values. Now I’m actually gonna ta lk about our first cl assification problem, \\nwhere the value Y you’re trying to predict will be discreet value. You can take on only a \\nsmall number of discrete values and in th is case I’ll talk about binding classification \\nwhere Y takes on only two values, right? So you  come up with classi fication problems if \\nyou’re trying to do, say, a medical diagnosis and try to decide based on some features that \\nthe patient has a disease or does not have a di sease. Or if in the housing example, maybe \\nyou’re trying to decide will this house sell in the next six months or not and the answer is \\neither yes or no. It’ll either be  sold in the next six months or it won’t be. Other standing', metadata={'source': '../data/docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 10}),\n",
       " Document(page_content='answer. You predict that if X is to the right of, sort of, the mid-point here then Y is one \\nand then next to the left of that mid-point then Y is zero.  \\nSo some people actually do this. Apply linear  regression to classi fication problems and \\nsometimes it’ll work okay, but in general it’s actually a pretty bad idea to apply linear \\nregression to classification problems like thes e and here’s why. Let’s say I change my \\ntraining set by giving you just one more tr aining example all the way up there, right? \\nImagine if given this training set is actually  still entirely obvious  what the relationship \\nbetween X and Y is, right? It’s ju st – take this value as greate r than Y is one and it’s less \\nthen Y is zero. By giving you this additiona l training example it really shouldn’t change \\nanything. I mean, I didn’t really convey much  new information. There’s no surprise that \\nthis corresponds to Y equals one. But if you now  fit linear regression to this data set you \\nend up with a line that, I don’t know, maybe  looks like that, right? And now the', metadata={'source': '../data/docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 10})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a3f11fe",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': '../data/docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 14}\n",
      "{'source': '../data/docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 0}\n",
      "{'source': '../data/docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 10}\n",
      "{'source': '../data/docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 10}\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534b5ae7",
   "metadata": {},
   "source": [
    "Now all the retrieved documents are from Lecture 3! LLM was able to parse out the metadata, and also decide the relevant filter to pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17b3664",
   "metadata": {},
   "source": [
    "## Contextual Compression\n",
    "\n",
    "- Another approach for improving the quality of retrieved docs is compression.\n",
    "\n",
    "- Information most relevant to a query may be buried in a document with a lot of irrelevant text. \n",
    "\n",
    "- Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
    "\n",
    "- Contextual compression is meant to fix this. \n",
    "- `LLMChainExtractor` extracts the relevant parts of the documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cac8b1ca",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d682805b",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5304526",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b1658b8",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_type=\"mmr\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "262b5e89",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "\"MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to plot data. And it's sort of an extremely easy to learn tool to use for implementing a lot of learning algorithms.\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "\"And the student said, \"Oh, it was the MATLAB.\" So for those of you that don't know MATLAB yet, I hope you do learn it. It's not hard, and we'll actually have a short MATLAB tutorial in one of the discussion sections for those of you that don't know it.\"\n"
     ]
    }
   ],
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f584ac",
   "metadata": {},
   "source": [
    "## Other types of retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "553c431b",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import SVMRetriever, TFIDFRetriever\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6bcff1f",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "fpath = \"../data/docs/cs229_lectures/MachineLearning-Lecture01.pdf\"\n",
    "loader = PyPDFLoader(fpath)\n",
    "docs = loader.load()\n",
    "\n",
    "all_page_text = [doc.page_content for doc in docs]\n",
    "all_text = ' '.join(all_page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "13735779",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
    "splits = text_splitter.split_text(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "834e4126",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "svm_retriever = SVMRetriever.from_texts(splits, embeddings=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cb68c4",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "tfidf_retriever = TFIDFRetriever.from_texts(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048f4330",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "question = \"What are the major topics for this class?\"\n",
    "docs_svm = svm_retriever.get_relevant_documents(question)\n",
    "docs_svm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d643b8c",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "docs_tfidf = svm_retriever.get_relevant_documents(question)\n",
    "docs_tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe372cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
